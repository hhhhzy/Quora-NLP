{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT vs CL-BERT w/o Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,BertForSequenceClassification\n",
    "from datasets import load_dataset, ClassLabel, Value, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load evaluation dataset + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quora (C:\\Users\\52673\\.cache\\huggingface\\datasets\\quora\\default\\0.0.0\\36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n",
      "Using custom data configuration default\n",
      "Reusing dataset quora (C:\\Users\\52673\\.cache\\huggingface\\datasets\\quora\\default\\0.0.0\\36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n",
      "Loading cached processed dataset at C:\\Users\\52673\\.cache\\huggingface\\datasets\\quora\\default\\0.0.0\\36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04\\cache-58ce8a2d3d13bc51.arrow\n",
      "Loading cached processed dataset at C:\\Users\\52673\\.cache\\huggingface\\datasets\\quora\\default\\0.0.0\\36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04\\cache-077de9dbe62a5b12.arrow\n"
     ]
    }
   ],
   "source": [
    "# sampling the dataset for evaluation\n",
    "train_eval = load_dataset(\"quora\", split = 'train[50%:85%]')\n",
    "validation_eval = load_dataset(\"quora\", split = 'train[85%:100%]')\n",
    "\n",
    "# repeat all the preprocessing steps above\n",
    "new_features = train_eval.features.copy()\n",
    "new_features[\"is_duplicate\"] = Value('int32')\n",
    "train_eval = train_eval.cast(new_features)\n",
    "\n",
    "new_features = validation_eval.features.copy()\n",
    "new_features[\"is_duplicate\"] = Value('int32')\n",
    "validation_eval = validation_eval.cast(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the metric module to compute accuracy and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = load_metric(\"accuracy\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_f1(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return f1.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer and model pretrained with bert\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141501/141501 [00:33<00:00, 4170.02ex/s]\n",
      "<ipython-input-6-7c80f4e8f9bf>:5: FutureWarning: rename_column_ is deprecated and will be removed in the next major version of datasets. Use Dataset.rename_column instead.\n",
      "  encoded_train_eval.rename_column_(\"is_duplicate\", \"labels\")\n",
      "100%|██████████| 60644/60644 [00:14<00:00, 4287.61ex/s]\n"
     ]
    }
   ],
   "source": [
    "# encode the training dataset in the form of sentences pair\n",
    "# truncate at length=64 for a balance of time consuming and information coverage\n",
    "encoded_train_eval = train_eval.map(lambda batch: bert_tokenizer(batch['questions']['text'][0], batch['questions']['text'][1], \n",
    "                                                  padding='max_length', truncation=True, max_length=64))\n",
    "encoded_train_eval.rename_column_(\"is_duplicate\", \"labels\")\n",
    "\n",
    "# encode the validation dataset in the form of sentences pair\n",
    "encoded_validation_eval = validation_eval.map(lambda batch: bert_tokenizer(batch['questions']['text'][0], batch['questions']['text'][1], \n",
    "                                                            padding='max_length', truncation=True, max_length=64))\n",
    "encoded_validation_eval.rename_column_(\"is_duplicate\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: questions.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60644\n",
      "  Batch size = 8\n",
      "100%|██████████| 7581/7581 [04:28<00:00, 28.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6913312673568726,\n",
       " 'eval_accuracy': 0.4584625024734516,\n",
       " 'eval_runtime': 269.5438,\n",
       " 'eval_samples_per_second': 224.988,\n",
       " 'eval_steps_per_second': 28.125}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_bert = Trainer(\n",
    "    model=bert,   \n",
    "    train_dataset=encoded_train_eval,\n",
    "    eval_dataset=encoded_validation_eval,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer_bert.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: questions.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60644\n",
      "  Batch size = 8\n",
      "100%|██████████| 7581/7581 [04:27<00:00, 28.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6913312673568726,\n",
       " 'eval_f1': 0.5665641621243517,\n",
       " 'eval_runtime': 267.3371,\n",
       " 'eval_samples_per_second': 226.845,\n",
       " 'eval_steps_per_second': 28.357}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_bert = Trainer(\n",
    "    model=bert,    \n",
    "    train_dataset=encoded_train_eval,\n",
    "    eval_dataset=encoded_validation_eval,\n",
    "    compute_metrics=compute_f1,\n",
    ")\n",
    "\n",
    "trainer_bert.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CL-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\09102786ff74bdc2d32e48fb8505b1d86fd33b33c1e1f149322505c3fcc8926e.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/special_tokens_map.json from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\8c406286308d13c3a53bc10c3d1a2d5113d4e46a34cb6ec5ee06e5d9762c462c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\4f2880ff62576ab971eea56ed4efbe8766ec79f9b35011e7ee8260a7feb608b8.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\886dba277a27c6ab50ab3d0bfd8839d354cfeed717289623026415c62b687338.1b14bcddba43d86a607eedb4b638b87d30aa00c839358953dbd36f2cd3317c83\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"result/bert-base-uncased-cls_before_pooler-sym_mlp-mlp_bert-bs64-gpu8-gs1-lr5e-5-m=stsb-norm0.05-l32-contra\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\52673/.cache\\huggingface\\transformers\\4c860a500382bddac6cccf88a682fd0a1bda5b282522cb42f1144306aa172416.ccdfe081eeadaaa135da8becf0290d82d3956ea27e3929311aa3985f3a5a320d\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer and model pretrained with CL-bert\n",
    "cl_tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "cl = BertForSequenceClassification.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141501/141501 [00:35<00:00, 3972.36ex/s]\n",
      "100%|██████████| 60644/60644 [00:14<00:00, 4101.05ex/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_train_eval = train_eval.map(lambda batch: cl_tokenizer(batch['questions']['text'][0], batch['questions']['text'][1], \n",
    "                                                  padding='max_length', truncation=True, max_length=64))\n",
    "encoded_train_eval.rename_column_(\"is_duplicate\", \"labels\")\n",
    "\n",
    "# encode the validation dataset in the form of sentences pair\n",
    "encoded_validation_eval = validation_eval.map(lambda batch: cl_tokenizer(batch['questions']['text'][0], batch['questions']['text'][1], \n",
    "                                                            padding='max_length', truncation=True, max_length=64))\n",
    "encoded_validation_eval.rename_column_(\"is_duplicate\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for CL-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: questions.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60644\n",
      "  Batch size = 8\n",
      "100%|██████████| 7581/7581 [04:30<00:00, 27.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7091943025588989,\n",
       " 'eval_accuracy': 0.4153584855880219,\n",
       " 'eval_runtime': 271.0187,\n",
       " 'eval_samples_per_second': 223.763,\n",
       " 'eval_steps_per_second': 27.972}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_cl = Trainer(\n",
    "    model=cl,    \n",
    "    train_dataset=encoded_train_eval,\n",
    "    eval_dataset=encoded_validation_eval,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer_cl.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 for CL-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: questions.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60644\n",
      "  Batch size = 8\n",
      "100%|██████████| 7581/7581 [04:43<00:00, 26.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7091943025588989,\n",
       " 'eval_f1': 0.4639968554885331,\n",
       " 'eval_runtime': 283.4258,\n",
       " 'eval_samples_per_second': 213.968,\n",
       " 'eval_steps_per_second': 26.748}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_cl = Trainer(\n",
    "    model=cl,   \n",
    "    train_dataset=encoded_train_eval,\n",
    "    eval_dataset=encoded_validation_eval,\n",
    "    compute_metrics=compute_f1,\n",
    ")\n",
    "\n",
    "trainer_cl.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01cce90487304195a98c1fadead284adb0ca16778f239acdcba59c3951b79f2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
